{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework solution. Week 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create virtual environment with conda:\n",
    "```\n",
    "conda create -n zoom_week_2 python=3.9\n",
    "conda activate zoom_week_2\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Load January 2020 data\n",
    "\n",
    "Using the `etl_web_to_gcs.py` flow that loads taxi data into GCS as a guide, create a flow that loads the green taxi CSV dataset for January 2020 into GCS and run it. Look at the logs to find out how many rows the dataset has.\n",
    "\n",
    "How many rows does that dataset have?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our modified script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:11:22.434 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'swinging-moth'\u001b[0m for flow\u001b[1;35m 'etl-web-to-gcs'\u001b[0m\n",
      "URL: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-01.csv.gz\n",
      "00:11:22.656 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Created task run 'fetch-b4598a4a-0' for task 'fetch'\n",
      "00:11:22.657 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Executing 'fetch-b4598a4a-0' immediately...\n",
      "/Users/a_kulesh/Workspace/education/courses/data-engineering-zoomcamp/_fork/data-engineering-zoomcamp/cohorts/2023/week_2_workflow_orchestration/etl_web_to_gcs.py:15: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(dataset_url)\n",
      "00:11:26.200 | \u001b[36mINFO\u001b[0m    | Task run 'fetch-b4598a4a-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:11:26.229 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Created task run 'clean-b9fd7e03-0' for task 'clean'\n",
      "00:11:26.230 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Executing 'clean-b9fd7e03-0' immediately...\n",
      "00:11:26.307 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' -    VendorID lpep_pickup_datetime  ... trip_type congestion_surcharge\n",
      "0       2.0  2019-12-18 15:52:30  ...       1.0                  0.0\n",
      "1       2.0  2020-01-01 00:45:58  ...       2.0                  0.0\n",
      "\n",
      "[2 rows x 20 columns]\n",
      "00:11:26.608 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - rows: 447770\n",
      "00:11:26.611 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - columns: VendorID                        float64\n",
      "lpep_pickup_datetime     datetime64[ns]\n",
      "lpep_dropoff_datetime    datetime64[ns]\n",
      "store_and_fwd_flag               object\n",
      "RatecodeID                      float64\n",
      "PULocationID                      int64\n",
      "DOLocationID                      int64\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "ehail_fee                       float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "payment_type                    float64\n",
      "trip_type                       float64\n",
      "congestion_surcharge            float64\n",
      "dtype: object\n",
      "00:11:26.639 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:11:26.672 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Created task run 'write_local-f322d1be-0' for task 'write_local'\n",
      "00:11:26.673 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Executing 'write_local-f322d1be-0' immediately...\n",
      "00:11:28.238 | \u001b[36mINFO\u001b[0m    | Task run 'write_local-f322d1be-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "Data saved on the local disk: data/green/green_tripdata_2020-01.parquet\n",
      "00:11:28.269 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Created task run 'write_gcs-1145c921-0' for task 'write_gcs'\n",
      "00:11:28.270 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Executing 'write_gcs-1145c921-0' immediately...\n",
      "00:11:28.485 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Getting bucket 'dtc_data_lake_virtual-dynamo-375412'.\n",
      "00:11:28.962 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Uploading from 'data/green/green_tripdata_2020-01.parquet' to the bucket 'dtc_data_lake_virtual-dynamo-375412' path 'data/green/green_tripdata_2020-01.parquet'.\n",
      "Destination on GCS: data/green/green_tripdata_2020-01.parquet\n",
      "00:11:32.216 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:11:32.248 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'swinging-moth'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python etl_web_to_gcs.py --color=green --year=2020 --month=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: 447770"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Scheduling with Cron\n",
    "\n",
    "Cron is a common scheduling specification for workflows. \n",
    "\n",
    "Using the flow in `etl_web_to_gcs.py`, create a deployment to run on the first of every month at 5am UTC. What’s the cron schedule for that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mFound flow 'etl-web-to-gcs'\u001b[0m\n",
      "\u001b[32mDeployment YAML created at \u001b[0m\n",
      "\u001b[32m'/Users/a_kulesh/Workspace/education/courses/data-engineering-zoomcamp/_fork/dat\u001b[0m\n",
      "\u001b[32ma-engineering-zoomcamp/cohorts/2023/week_2_workflow_orchestration/etl_web_to_gcs\u001b[0m\n",
      "\u001b[32m-deployment.yaml'.\u001b[0m\n",
      "\u001b[32mDeployment storage None does not have upload capabilities; no files uploaded.  \u001b[0m\n",
      "\u001b[32mPass --skip-upload to suppress this warning.\u001b[0m\n",
      "\u001b[32mSuccessfully loaded 'My Taxi Flow'\u001b[0m\n",
      "\u001b[32mDeployment 'etl-web-to-gcs/My Taxi Flow' successfully created with id \u001b[0m\n",
      "\u001b[32m'cef6a1b9-faef-46d2-be09-6b14d469deef'.\u001b[0m\n",
      "\n",
      "To execute flow runs from this deployment, start an agent that pulls work from \n",
      "the 'default' work queue:\n",
      "\u001b[34m$ prefect agent start -q 'default'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect deployment build ./etl_web_to_gcs.py:etl_web_to_gcs -n \"My Taxi Flow\" --cron \"0 5 1 * *\"\n",
    "!prefect deployment apply ./etl_web_to_gcs-deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Loading data to BigQuery \n",
    "\n",
    "Using `etl_gcs_to_bq.py` as a starting point, modify the script for extracting data from GCS and loading it into BigQuery. This new script should not fill or remove rows with missing values. (The script is really just doing the E and L parts of ETL).\n",
    "\n",
    "The main flow should print the total number of rows processed by the script. Set the flow decorator to log the print statement.\n",
    "\n",
    "Parametrize the entrypoint flow to accept a list of months, a year, and a taxi color. \n",
    "\n",
    "Make any other necessary changes to the code for it to function as required.\n",
    "\n",
    "Create a deployment for this flow to run in a local subprocess with local flow code storage (the defaults).\n",
    "\n",
    "Make sure you have the parquet data files for Yellow taxi data for Feb. 2019 and March 2019 loaded in GCS. Run your deployment to append this data to your BiqQuery table. How many rows did your flow code process?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading data to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:29:48.267 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'manipulative-crayfish'\u001b[0m for flow\u001b[1;35m 'etl-web-to-gcs'\u001b[0m\n",
      "URL: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-02.csv.gz\n",
      "00:29:48.497 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Created task run 'fetch-b4598a4a-0' for task 'fetch'\n",
      "00:29:48.500 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Executing 'fetch-b4598a4a-0' immediately...\n",
      "00:33:08.447 | \u001b[36mINFO\u001b[0m    | Task run 'fetch-b4598a4a-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:33:08.480 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Created task run 'clean-b9fd7e03-0' for task 'clean'\n",
      "00:33:08.481 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Executing 'clean-b9fd7e03-0' immediately...\n",
      "00:33:08.575 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' -    VendorID tpep_pickup_datetime  ... total_amount  congestion_surcharge\n",
      "0         1  2019-02-01 00:59:04  ...         12.3                   0.0\n",
      "1         1  2019-02-01 00:33:09  ...         33.3                   0.0\n",
      "\n",
      "[2 rows x 18 columns]\n",
      "00:33:08.577 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - rows: 7019375\n",
      "00:33:08.580 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - columns: VendorID                   int64\n",
      "tpep_pickup_datetime      object\n",
      "tpep_dropoff_datetime     object\n",
      "passenger_count            int64\n",
      "trip_distance            float64\n",
      "RatecodeID                 int64\n",
      "store_and_fwd_flag        object\n",
      "PULocationID               int64\n",
      "DOLocationID               int64\n",
      "payment_type               int64\n",
      "fare_amount              float64\n",
      "extra                    float64\n",
      "mta_tax                  float64\n",
      "tip_amount               float64\n",
      "tolls_amount             float64\n",
      "improvement_surcharge    float64\n",
      "total_amount             float64\n",
      "congestion_surcharge     float64\n",
      "dtype: object\n",
      "00:33:08.607 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:33:08.640 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Created task run 'write_local-f322d1be-0' for task 'write_local'\n",
      "00:33:08.641 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Executing 'write_local-f322d1be-0' immediately...\n",
      "00:33:36.212 | \u001b[36mINFO\u001b[0m    | Task run 'write_local-f322d1be-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "Data saved on the local disk: data/yellow/yellow_tripdata_2019-02.parquet\n",
      "00:33:36.249 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Created task run 'write_gcs-1145c921-0' for task 'write_gcs'\n",
      "00:33:36.249 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Executing 'write_gcs-1145c921-0' immediately...\n",
      "00:33:36.449 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Getting bucket 'dtc_data_lake_virtual-dynamo-375412'.\n",
      "00:33:36.946 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Uploading from 'data/yellow/yellow_tripdata_2019-02.parquet' to the bucket 'dtc_data_lake_virtual-dynamo-375412' path 'data/yellow/yellow_tripdata_2019-02.parquet'.\n",
      "Destination on GCS: data/yellow/yellow_tripdata_2019-02.parquet\n",
      "00:34:20.315 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:34:20.380 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'manipulative-crayfish'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n",
      "00:34:26.195 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'taupe-bullfrog'\u001b[0m for flow\u001b[1;35m 'etl-web-to-gcs'\u001b[0m\n",
      "URL: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-03.csv.gz\n",
      "00:34:26.372 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Created task run 'fetch-b4598a4a-0' for task 'fetch'\n",
      "00:34:26.373 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Executing 'fetch-b4598a4a-0' immediately...\n",
      "00:38:11.595 | \u001b[36mINFO\u001b[0m    | Task run 'fetch-b4598a4a-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:38:11.649 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Created task run 'clean-b9fd7e03-0' for task 'clean'\n",
      "00:38:11.650 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Executing 'clean-b9fd7e03-0' immediately...\n",
      "00:38:11.763 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' -    VendorID tpep_pickup_datetime  ... total_amount  congestion_surcharge\n",
      "0         1  2019-03-01 00:24:41  ...          3.8                   0.0\n",
      "1         1  2019-03-01 00:25:27  ...         15.0                   0.0\n",
      "\n",
      "[2 rows x 18 columns]\n",
      "00:38:11.765 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - rows: 7832545\n",
      "00:38:11.767 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - columns: VendorID                   int64\n",
      "tpep_pickup_datetime      object\n",
      "tpep_dropoff_datetime     object\n",
      "passenger_count            int64\n",
      "trip_distance            float64\n",
      "RatecodeID                 int64\n",
      "store_and_fwd_flag        object\n",
      "PULocationID               int64\n",
      "DOLocationID               int64\n",
      "payment_type               int64\n",
      "fare_amount              float64\n",
      "extra                    float64\n",
      "mta_tax                  float64\n",
      "tip_amount               float64\n",
      "tolls_amount             float64\n",
      "improvement_surcharge    float64\n",
      "total_amount             float64\n",
      "congestion_surcharge     float64\n",
      "dtype: object\n",
      "00:38:11.803 | \u001b[36mINFO\u001b[0m    | Task run 'clean-b9fd7e03-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:38:11.851 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Created task run 'write_local-f322d1be-0' for task 'write_local'\n",
      "00:38:11.853 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Executing 'write_local-f322d1be-0' immediately...\n",
      "00:38:44.374 | \u001b[36mINFO\u001b[0m    | Task run 'write_local-f322d1be-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "Data saved on the local disk: data/yellow/yellow_tripdata_2019-03.parquet\n",
      "00:38:44.417 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Created task run 'write_gcs-1145c921-0' for task 'write_gcs'\n",
      "00:38:44.418 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Executing 'write_gcs-1145c921-0' immediately...\n",
      "00:38:44.655 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Getting bucket 'dtc_data_lake_virtual-dynamo-375412'.\n",
      "00:38:45.259 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Uploading from 'data/yellow/yellow_tripdata_2019-03.parquet' to the bucket 'dtc_data_lake_virtual-dynamo-375412' path 'data/yellow/yellow_tripdata_2019-03.parquet'.\n",
      "Destination on GCS: data/yellow/yellow_tripdata_2019-03.parquet\n",
      "00:39:29.977 | \u001b[36mINFO\u001b[0m    | Task run 'write_gcs-1145c921-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:39:30.013 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'taupe-bullfrog'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python etl_web_to_gcs.py --color=yellow --year=2019 --month=2\n",
    "!python etl_web_to_gcs.py --color=yellow --year=2019 --month=3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading data from GCS to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:51:10.998 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'fantastic-jacamar'\u001b[0m for flow\u001b[1;35m 'etl-gcs-to-bq'\u001b[0m\n",
      "00:51:11.201 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Created task run 'extract_from_gcs-968e3b65-0' for task 'extract_from_gcs'\n",
      "00:51:11.203 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Executing 'extract_from_gcs-968e3b65-0' immediately...\n",
      "00:51:11.909 | \u001b[36mINFO\u001b[0m    | Task run 'extract_from_gcs-968e3b65-0' - Downloading blob named data/yellow/yellow_tripdata_2019-02.parquet from the dtc_data_lake_virtual-dynamo-375412 bucket to data/gcs/data/yellow/yellow_tripdata_2019-02.parquet/data/yellow/yellow_tripdata_2019-02.parquet\n",
      "00:51:27.658 | \u001b[36mINFO\u001b[0m    | Task run 'extract_from_gcs-968e3b65-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:51:27.690 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Created task run 'transform-a7d916b4-0' for task 'transform'\n",
      "00:51:27.691 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Executing 'transform-a7d916b4-0' immediately...\n",
      "00:51:32.720 | \u001b[36mINFO\u001b[0m    | Task run 'transform-a7d916b4-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:51:32.722 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Total number of rows: 7019375\n",
      "00:51:32.769 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Created task run 'write_bq-b366772c-0' for task 'write_bq'\n",
      "00:51:32.771 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Executing 'write_bq-b366772c-0' immediately...\n",
      "00:53:32.385 | \u001b[36mINFO\u001b[0m    | Task run 'write_bq-b366772c-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:53:32.425 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Created task run 'extract_from_gcs-968e3b65-1' for task 'extract_from_gcs'\n",
      "00:53:32.426 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Executing 'extract_from_gcs-968e3b65-1' immediately...\n",
      "00:53:33.192 | \u001b[36mINFO\u001b[0m    | Task run 'extract_from_gcs-968e3b65-1' - Downloading blob named data/yellow/yellow_tripdata_2019-03.parquet from the dtc_data_lake_virtual-dynamo-375412 bucket to data/gcs/data/yellow/yellow_tripdata_2019-03.parquet/data/yellow/yellow_tripdata_2019-03.parquet\n",
      "00:53:50.730 | \u001b[36mINFO\u001b[0m    | Task run 'extract_from_gcs-968e3b65-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:53:50.776 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Created task run 'transform-a7d916b4-1' for task 'transform'\n",
      "00:53:50.777 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Executing 'transform-a7d916b4-1' immediately...\n",
      "00:53:55.942 | \u001b[36mINFO\u001b[0m    | Task run 'transform-a7d916b4-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:53:55.945 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Total number of rows: 14851920\n",
      "00:53:55.974 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Created task run 'write_bq-b366772c-1' for task 'write_bq'\n",
      "00:53:55.975 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Executing 'write_bq-b366772c-1' immediately...\n",
      "00:56:08.363 | \u001b[36mINFO\u001b[0m    | Task run 'write_bq-b366772c-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "00:56:08.403 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'fantastic-jacamar'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python etl_gcs_to_bq.py --color=yellow --year=2019 --months=2,3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Github Storage Block\n",
    "\n",
    "Using the `web_to_gcs` script from the videos as a guide, you want to store your flow code in a GitHub repository for collaboration with your team. Prefect can look in the GitHub repo to find your flow code and read it. Create a GitHub storage block from the UI or in Python code and use that in your Deployment instead of storing your flow code locally or baking your flow code into a Docker image. \n",
    "\n",
    "Note that you will have to push your code to GitHub, Prefect will not push it for you.\n",
    "\n",
    "Run your deployment in a local subprocess (the default if you don’t specify an infrastructure). Use the Green taxi data for the month of November 2020.\n",
    "\n",
    "How many rows were processed by the script?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Email or Slack notifications\n",
    "\n",
    "Q5. It’s often helpful to be notified when something with your dataflow doesn’t work as planned. Choose one of the options below for creating email or slack notifications.\n",
    "\n",
    "The hosted Prefect Cloud lets you avoid running your own server and has Automations that allow you to get notifications when certain events occur or don’t occur. \n",
    "\n",
    "Create a free forever Prefect Cloud account at app.prefect.cloud and connect your workspace to it following the steps in the UI when you sign up. \n",
    "\n",
    "Set up an Automation that will send yourself an email when a flow run completes. Run the deployment used in Q4 for the Green taxi data for April 2019. Check your email to see the notification.\n",
    "\n",
    "Alternatively, use a Prefect Cloud Automation or a self-hosted Orion server Notification to get notifications in a Slack workspace via an incoming webhook. \n",
    "\n",
    "Join my temporary Slack workspace with [this link](https://join.slack.com/t/temp-notify/shared_invite/zt-1odklt4wh-hH~b89HN8MjMrPGEaOlxIw). 400 people can use this link and it expires in 90 days. \n",
    "\n",
    "In the Prefect Cloud UI create an [Automation](https://docs.prefect.io/ui/automations) or in the Prefect Orion UI create a [Notification](https://docs.prefect.io/ui/notifications/) to send a Slack message when a flow run enters a Completed state. Here is the Webhook URL to use: https://hooks.slack.com/services/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\n",
    "\n",
    "Test the functionality.\n",
    "\n",
    "Alternatively, you can grab the webhook URL from your own Slack workspace and Slack App that you create. \n",
    "\n",
    "\n",
    "How many rows were processed by the script?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Secrets\n",
    "\n",
    "Prefect Secret blocks provide secure, encrypted storage in the database and obfuscation in the UI. Create a secret block in the UI that stores a fake 10-digit password to connect to a third-party service. Once you’ve created your block in the UI, how many characters are shown as asterisks (*) on the next page of the UI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoom_week_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adcca963d246f2f9e6af088acaba4f4286bc7ad829faf4741c61865aebf536f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
